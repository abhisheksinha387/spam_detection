---

# üìß Spam Detection Project

A machine learning-based web application built with **Flask** to classify text messages as **spam** or **ham** (non-spam). This project uses a **Support Vector Machine (SVM)** model trained on **SentenceTransformer** embeddings and SpaCy-based text features.

---

## üìö Table of Contents

* [Project Overview](#project-overview)
* [Features](#features)
* [Project Structure](#project-structure)
* [Prerequisites](#prerequisites)
* [Local Setup](#local-setup)
* [Running the Application Locally](#running-the-application-locally)
* [Deploying on Render](#deploying-on-render)
* [Dataset](#dataset)
* [Contributing](#contributing)
* [License](#license)

---

## üîç Project Overview

This project implements a spam detection system that:

* Ingests a dataset of text messages labeled as spam or ham.
* Processes data with SentenceTransformer embeddings and SpaCy text features.
* Trains a Linear SVM classifier with GridSearchCV for tuning.
* Offers a Flask-based UI for text input and prediction.
* Modular design: ingestion, transformation, training, and prediction pipelines.

---

## ‚ú® Features

* **Data Ingestion**: Loads and splits the dataset.
* **Text Embeddings**: Uses `all-MiniLM-L6-v2` from SentenceTransformer.
* **Feature Engineering**: Character, word, and sentence count via SpaCy.
* **Model Training**: Linear SVM with hyperparameter tuning using GridSearchCV.
* **Web Interface**: Clean, minimal Flask UI for predictions.
* **Logging**: Custom logs for better debugging.
* **Deployment Ready**: Easily deploy on Render.

---

## üìÅ Project Structure

```
spam_detection/
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ spam.csv                 # Dataset file
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.py       # Data loading & splitting
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_transformation.py  # Embeddings & feature extraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_trainer.py        # Model training & evaluation
‚îÇ   ‚îú‚îÄ‚îÄ pipeline/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predict_pipeline.py     # Prediction logic for web app
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_pipeline.py       # Full training pipeline
‚îÇ   ‚îú‚îÄ‚îÄ exception.py                # Custom exceptions
‚îÇ   ‚îú‚îÄ‚îÄ logger.py                   # Logging utility
‚îÇ   ‚îú‚îÄ‚îÄ utils.py                    # Utility functions
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ index.html              # HTML template for Flask UI
‚îú‚îÄ‚îÄ app.py                      # Flask web app entry point
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ setup.py                    # Setup file
‚îú‚îÄ‚îÄ .gitignore                  # Ignored files
‚îú‚îÄ‚îÄ README.md                   # This file
```

> **Note:** `logs/` and `artifacts/` are generated at runtime and ignored by Git.

---

## ‚öôÔ∏è Prerequisites

* Python 3.8+
* Git
* A GitHub account
* (Optional) Render account for deployment
* `spam.csv` dataset

---

## üõ†Ô∏è Local Setup

### 1. Clone the Repository

```bash
git clone https://github.com/<your-username>/spam-detection.git
cd spam-detection
```

### 2. Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate      # Windows: venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

### 4. Prepare Dataset

Place `spam.csv` in the `notebooks/` folder with:

* `v1` (labels: ham/spam)
* `v2` (text messages)

Alternatively, update `data_ingestion.py` to download from a URL.

---

### 5. Run the Training Pipeline

```bash
python -m src.pipeline.train_pipeline
```

Generates `artifacts/`:

* `model.pkl`, `preprocessor.pkl`
* `train.csv`, `test.csv`, `data.csv`
* `train_embeddings.csv`, `test_embeddings.csv`
* `train_target.csv`, `test_target.csv`

---

## üöÄ Running the Application Locally

### Start the Flask App

```bash
python app.py
```

### Access the App

Open [http://localhost:5000](http://localhost:5000) in your browser to:

* Enter a message
* Click **Predict**
* View spam or ham result

### Logs

Check `logs/` for runtime logs and errors.

---

## üåê Deploying on Render

### 1. Push to GitHub

```bash
git add .
git commit -m "Prepare for Render deployment"
git push origin main
```

> Ensure `artifacts/` and `logs/` are excluded via `.gitignore`.

### 2. Create Web Service on Render

* Go to Render ‚Üí **New** ‚Üí **Web Service**
* Connect GitHub repo
* Configure:

  * **Name**: `spam-detection-app`
  * **Runtime**: Python
  * **Branch**: `main`
  * **Build Command**:

    ```bash
    pip install -r requirements.txt && python -m spacy download en_core_web_sm
    ```
  * **Start Command**:

    ```bash
    python app.py
    ```
  * **Instance Type**: Free or upgrade for persistence

### 3. Handle Artifacts and Dataset

#### ‚úÖ Option 1: Commit Artifacts

* Temporarily remove `artifacts/` from `.gitignore`
* Commit `model.pkl`, `preprocessor.pkl`, etc.

#### ‚úÖ Option 2: Use Render Disk

* Add disk in Render dashboard:

  * Name: `artifacts-disk`
  * Mount Path: `/app/artifacts`
  * Size: 1GB

**Update paths in code:**

```python
# data_ingestion.py
train_data_path = os.path.join('/app/artifacts', 'train.csv')
...
# data_transformation.py
preprocessor_obj_file_path = os.path.join('/app/artifacts', 'preprocessor.pkl')
...
```

**Update Build Command:**

```bash
pip install -r requirements.txt && python -m spacy download en_core_web_sm && cp notebooks/spam.csv /app/artifacts/spam.csv
```

**Update Start Command:**

```bash
python -m src.pipeline.train_pipeline && python app.py
```

#### ‚úÖ Option 3: Use Public URL for Dataset

Update `data_ingestion.py`:

```python
import requests
url = "https://your-public-url/spam.csv"
response = requests.get(url)
with open('temp_spam.csv', 'wb') as f:
    f.write(response.content)
df = pd.read_csv('temp_spam.csv', encoding='latin-1')
```

---

## üì¶ Dataset

* **Columns**: `v1` (ham/spam), `v2` (text)
* **Location**: `notebooks/spam.csv` or hosted URL
* **Source**: [Kaggle - SMS Spam Collection](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset)

---



